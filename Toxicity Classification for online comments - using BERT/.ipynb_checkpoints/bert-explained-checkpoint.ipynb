{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all dependencies \n",
    "Apart from the standard numpy, pandas and tensorflow libraries, also download the BERT files which contains models which were pretriained models, which will be used for fine tuning in this kernel. \n",
    "\n",
    "Also, thanks to https://www.kaggle.com/taindow/bert-a-fine-tuning-example \n",
    "\n",
    "reference : https://github.com/google-research/bert\n",
    "\n",
    "@article{devlin2018bert,\n",
    "  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n",
    "  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
    "  journal={arXiv preprint arXiv:1810.04805},\n",
    "  year={2018}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jigsaw-unintended-bias-in-toxicity-classification', 'pretrained-bert-including-scripts']\n",
      "TensorFlow version:  1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from run_classifier import *\n",
    "import modeling\n",
    "import run_classifier\n",
    "import optimization\n",
    "import tokenization\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "* Upload all the data\n",
    "* Add a new column called labels - any target score > 0.5 is assigned label 1 else 0\n",
    "* Use only 30% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing text\n",
    "* Split train set into train and evalaution sets\n",
    "* Remove stopwords and limit the length of sentence to 72 words in train, dev and test sets\n",
    "* Convert to .tsv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Stopwords(pd_frame):\n",
    "    stop_words = {'not', 'won', 'but', 'this', 'most', \"isn't\", 'have', 'just', 'themselves', 'too', 'isn', 'hadn', 'about', 'from', 'both', \"don't\", 'hasn', \"mightn't\", 'your', 'his', 'than', 'so', 'now', 'until', 'they', 'ain', 'does', 'itself', 'or', 'off', \"aren't\", 'haven', 'i', 'you', 'he', 'why', 'it', 'under', 'd', 'mightn', 'up', 'each', 'down', 'y', 'o', 're', 'wouldn', \"should've\", 'no', 'which', 'aren', 'a', \"you'll\", \"mustn't\", 'doing', \"didn't\", 'same', 't', 'whom', \"shan't\", 'an', 'don', \"wasn't\", 'its', 'those', 'own', 'yours', 'myself', 'and', 'has', 'wasn', 'll', \"hasn't\", 'was', 'in', 'few', 'other', \"couldn't\", 'then', 'be', 'being', 'nor', \"needn't\", 'can', \"won't\", 'couldn', 'weren', 'been', 'for', \"shouldn't\", 'there', 'needn', 'yourself', 'how', 'her', 'herself', 'below', \"you're\", 'when', 'very', \"haven't\", 'into', 'didn', 'them', 'to', 'above', 'shan', 'some', 'are', 'on', 'is', 'their', 'at', 'am', 'hers', 'doesn', 'between', 'while', 's', 'should', 'theirs', 'himself', \"that'll\", 'ours', 'yourselves', 'what', 'again', 'had', 'ma', 'our', \"you'd\", 'my', 'out', \"she's\", 'she', 'if', \"weren't\", 'that', 'these', 'will', 'with', 'against', 'do', 'ourselves', 'all', 'who', 'as', 'here', \"wouldn't\", 've', 'through', 'the', 'after', \"hadn't\", 'of', 'having', 'once', 'only', 'because', 'where', \"it's\", 'by', 'shouldn', \"doesn't\", 'we', 'during', 'over', 'any', \"you've\", 'him', 'me', 'more', 'did', 'further', 'such', 'm', 'mustn', 'before', 'were'}\n",
    "    list_sent = []\n",
    "    for i in tqdm(range(len(pd_frame))):\n",
    "        new_str = ''\n",
    "        tot_length = 70\n",
    "        sent = pd_frame.iloc[i]\n",
    "        for j in sent.split():\n",
    "            if j not in stop_words:\n",
    "                new_str += ' ' + j.lower()\n",
    "        list_sent.append(new_str)\n",
    "    new_pd = pd.DataFrame(list_sent)\n",
    "    return list_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 902437/902437 [00:32<00:00, 27622.68it/s]\n",
      "100%|██████████| 297805/297805 [00:11<00:00, 26549.16it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "100%|██████████| 97320/97320 [00:03<00:00, 26062.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:32.331944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train['comment_text'] = train['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "\n",
    "test['comment_text'] = test['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "\n",
    "train['dummy_1'] = 'meh'\n",
    "train['dummy_2'] = '*'\n",
    "train = train[['dummy_1','target','dummy_2','comment_text']]\n",
    "train['target'] = np.where(train['target']>=0.5,1,0)\n",
    "\n",
    "train_set = train.sample(frac=0.5,random_state = 42)\n",
    "train, dev, y_train, y_test = train_test_split(train_set, train_set['target'], test_size=0.33, random_state=42)\n",
    "train = train_set\n",
    "train['comment_text'] = remove_Stopwords(train['comment_text'])\n",
    "dev['comment_text'] = remove_Stopwords(dev['comment_text'])\n",
    "test['comment_text'] = remove_Stopwords(test['comment_text'])\n",
    "\n",
    "print(datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.tsv', sep='\\t', index=False, header=False)\n",
    "dev.to_csv('dev.tsv', sep='\\t', index=False, header=False)\n",
    "test.to_csv('test.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of train and test file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        dummy_1                        ...                                                               comment_text\n",
      "286892      meh                        ...                           what breathe fresh air someone embraces commo...\n",
      "419218      meh                        ...                           your jewish friends ones told zionists contro...\n",
      "1055330     meh                        ...                           possible collusion trump affiliates debunked,...\n",
      "1382764     meh                        ...                           exactly. we need % gdp spending cap federal l...\n",
      "256049      meh                        ...                           by comment, even voted ndp pq, trudeau demons...\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "        dummy_1                        ...                                                               comment_text\n",
      "762659      meh                        ...                           \"the author quite correct saying real economy...\n",
      "303985      meh                        ...                           which dismisses may worked swing shift take f...\n",
      "77940       meh                        ...                                         why would he want replace gabbard?\n",
      "1452511     meh                        ...                           shame right longer believe compassion. so sho...\n",
      "283497      meh                        ...                           yes, bill's comments spot on. the current sit...\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "        id                                       comment_text\n",
      "0  7097320   [ integrity means pay debts.] does apply pres...\n",
      "1  7097321   this malfeasance administrator board. they wa...\n",
      "2  7097322   @rmiller101 - spoken like true elitist. but l...\n",
      "3  7097323   paul: thank kind words. i do, indeed, strong ...\n",
      "4  7097324   sorry missed high school. eisenhower sent tro...\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "print(dev.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data into BERT acceptable format\n",
    " Bert has a processor to convert comment_text and corresponding target value into the InputExample class(with attributes: text_a, labels, guid). In order to use BERT we need to convert our input training examples into InputExample class method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    BERT_BASE_DIR='../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "    vocab_file=BERT_BASE_DIR+'vocab.txt'\n",
    "    bert_config_file=BERT_BASE_DIR+'bert_config.json'\n",
    "    init_checkpoint=BERT_BASE_DIR+'bert_model.ckpt'\n",
    "    output_dir = \"./\"\n",
    "    do_lower_case = True\n",
    "    max_seq_length = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ColaProcessor()\n",
    "label_list = processor.get_labels()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LENGTH = 72\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "tpu_cluster_resolver = None\n",
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "use_tpu = False\n",
    "master = None \n",
    "save_checkpoints_steps = 99999999\n",
    "iterations_per_loop = 1000\n",
    "num_tpu_cores = 8\n",
    "tpu_cluster_resolver = None\n",
    "output_dir = './'\n",
    "data_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from 'comment_text'\n",
    "Extract features form comment_text for train,test and dev files and convert into a tf_record file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:26:07.719760\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_examples = processor.get_train_examples('./')\n",
    "train_file = os.path.join(output_dir, \"train.tf_record\")\n",
    "file_based_convert_examples_to_features(train_examples, label_list,\n",
    "                                        MAX_SEQ_LENGTH, tokenizer,\n",
    "                                        train_file)\n",
    "dev_examples = processor.get_dev_examples('./')\n",
    "dev_file = os.path.join(output_dir, \"dev.tf_record\")\n",
    "file_based_convert_examples_to_features(dev_examples, label_list,\n",
    "                                        MAX_SEQ_LENGTH, tokenizer,\n",
    "                                        dev_file)\n",
    "predict_examples = processor.get_test_examples('./')\n",
    "predict_file = os.path.join(output_dir, \"predict.tf_record\")\n",
    "file_based_convert_examples_to_features(predict_examples, label_list,\n",
    "                                        MAX_SEQ_LENGTH, tokenizer,\n",
    "                                        predict_file)\n",
    "print(datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.contrib.tpu.RunConfig(cluster=tpu_cluster_resolver, master=master, model_dir=FLAGS.output_dir, save_checkpoints_steps=save_checkpoints_steps,\n",
    "                                      tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_tpu_cores,\n",
    "                                      per_host_input_for_training=is_per_host))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:43:23.478672\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "  cluster=tpu_cluster_resolver,\n",
    "  master=master,\n",
    "  model_dir=FLAGS.output_dir,\n",
    "  save_checkpoints_steps=save_checkpoints_steps,\n",
    "  tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "      iterations_per_loop=iterations_per_loop,\n",
    "      num_shards=num_tpu_cores,\n",
    "      per_host_input_for_training=is_per_host))\n",
    "\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      num_labels=len(label_list),\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu=use_tpu,\n",
    "      use_one_hot_embeddings=use_tpu)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=32)\n",
    "      \n",
    "\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "    \n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(datetime.now() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    \n",
    "    #batch_size = params[\"batch_size\"]\n",
    "    batch_size = 64 # <----- hardcoded batch_size added here \n",
    "    \n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev_examples = processor.get_dev_examples(data_dir)\n",
    "num_actual_dev_examples = len(dev_examples)\n",
    "dev_drop_remainder = False\n",
    "dev_batch_size = 32\n",
    "tf.logging.info(\"***** Running evalua`tion*****\")\n",
    "tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
    "                len(dev_examples), num_actual_dev_examples,\n",
    "                len(dev_examples) - num_actual_dev_examples)\n",
    "tf.logging.info(\"  Batch size = %d\", dev_batch_size)\n",
    "\n",
    "dev_drop_remainder = True if use_tpu else False\n",
    "dev_input_fn = file_based_input_fn_builder(\n",
    "    input_file=dev_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=dev_drop_remainder)\n",
    "\n",
    "estimator.evaluate(input_fn=dev_input_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.95725054,\n",
       " 'eval_loss': 0.11092799,\n",
       " 'loss': 0.11091082,\n",
       " 'global_step': 28201}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_examples = processor.get_dev_examples(data_dir)\n",
    "num_actual_dev_examples = len(dev_examples)\n",
    "dev_drop_remainder = False\n",
    "dev_batch_size = 32\n",
    "tf.logging.info(\"***** Running evalua`tion*****\")\n",
    "tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
    "                len(dev_examples), num_actual_dev_examples,\n",
    "                len(dev_examples) - num_actual_dev_examples)\n",
    "tf.logging.info(\"  Batch size = %d\", dev_batch_size)\n",
    "\n",
    "dev_drop_remainder = True if use_tpu else False\n",
    "dev_input_fn = file_based_input_fn_builder(\n",
    "    input_file=dev_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=dev_drop_remainder)\n",
    "\n",
    "estimator.evaluate(input_fn=dev_input_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_examples = processor.get_test_examples(data_dir)\n",
    "num_actual_predict_examples = len(predict_examples)\n",
    "predict_batch_size = 32\n",
    "predict_drop_remainder = True\n",
    "\n",
    "tf.logging.info(\"***** Running prediction*****\")\n",
    "tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
    "                len(predict_examples), num_actual_predict_examples,\n",
    "                len(predict_examples) - num_actual_predict_examples)\n",
    "tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
    "\n",
    "predict_drop_remainder = True if use_tpu else False\n",
    "predict_input_fn = file_based_input_fn_builder(\n",
    "    input_file=predict_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=predict_drop_remainder)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)\n",
    "\n",
    "output_predict_file = os.path.join(output_dir, \"test_results.tsv\")\n",
    "\n",
    "with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
    "    num_written_lines = 0\n",
    "    tf.logging.info(\"***** Predict results *****\")\n",
    "    for (i, prediction) in enumerate(result):\n",
    "        probabilities = prediction[\"probabilities\"]\n",
    "        if i >= num_actual_predict_examples:\n",
    "            break\n",
    "        output_line = \"\\t\".join(\n",
    "            str(class_probability)\n",
    "            for class_probability in probabilities) + \"\\n\"\n",
    "        writer.write(output_line)\n",
    "        num_written_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "predictions = pd.read_csv('./test_results.tsv', header=None, sep='\\t')\n",
    "\n",
    "submission = pd.concat([sample_submission.iloc[:,0], predictions.iloc[:,1]], axis=1)\n",
    "submission.columns = ['id','prediction']\n",
    "submission.to_csv('submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:00:28.952353\n"
     ]
    }
   ],
   "source": [
    "submission.head()\n",
    "print(datetime.now() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316.992px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
